{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "#Create a small validation set.\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "#Utility functions to map characters to vocabulary IDs and back.\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "#Function to generate a training batch for the LSTM model.\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 27)\n",
      "1562484\n",
      "26\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_batches.next()[1].shape)\n",
    "print(len(train_text) // batch_size)\n",
    "print(len(string.ascii_lowercase))\n",
    "print(np.zeros(shape=(2, 4), dtype=np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simple LSTM Model.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298169 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "tb hfcogegrhdl to fufrvxngow    ssodo xsgogcgcbhx  ecwnh i rsf hecills d tn uwtb\n",
      "xr  i rhryicfyeti zovabo ztd binobihe fdtzsetln skkqtn msxmmoilpcr odorrhdnyer i\n",
      "w denlxickrxm iioea tyeaiz t  hortnidco tfeu ge genysiuyptjaat gscire feyxwuvrov\n",
      "lapvrrkagatgednt l  fyxubdgeilgek  rtzkenfjne eu odrrief yuauwwwvermwa iqotcqdd \n",
      "rae fxzetxnvvs  laroadcag ainydbtmilavojbimjicsikt wrpae ebor gaeqmeaulj jeoqeev\n",
      "================================================================================\n",
      "Validation set perplexity: 19.93\n",
      "Average loss at step 100: 2.608129 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.98\n",
      "Validation set perplexity: 10.56\n",
      "Average loss at step 200: 2.264968 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.29\n",
      "Validation set perplexity: 9.09\n",
      "Average loss at step 300: 2.102529 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 400: 2.009385 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.932653 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 600: 1.898191 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 700: 1.852015 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 800: 1.825048 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 900: 1.783873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1000: 1.781021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "bol tho teesz of feaniou radal refoutraly saint inf to part of debeth may to in \n",
      "hat shicea s chanqned flaws the unoricating trom to thoy fo fourized is four ari\n",
      "jes the protaned staneflitic epodced iniplated datament diin rease is scimm the \n",
      "in chibicies nanotagid two nine seven zero zero zero zero zero zero zero zero al\n",
      "freen antoring respeenation produrs amloment prope sand and econoliame amaniogy \n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1100: 1.796385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1200: 1.775178 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.779996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1400: 1.748169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1500: 1.731871 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.726141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1700: 1.718882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1800: 1.693172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1900: 1.705008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2000: 1.728078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "pan battuact d interbiquted ex rues the evican jourcar maried fruaghing secuenos\n",
      "culllosid rishy as to like such onernishop are sabplence throughthreate finck re\n",
      "ing belact shiplence werh to lity as the to the injaphumed be and tabriartain in\n",
      "s time are this maniamed the potession a subther praced reperversed preequent in\n",
      "dess and the secame to namply absolus theirno x one motoous forcanu parth and th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2100: 1.681476 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2200: 1.663474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2300: 1.681233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2400: 1.688795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2500: 1.693035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2600: 1.711495 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2700: 1.650535 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2800: 1.668460 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2900: 1.650741 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3000: 1.673894 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "ellette tricked involtultion descript tocceish compms from presbitt defugensery \n",
      "combobing with largive unment ration or clandsing the wavile servious in the rec\n",
      "king troluted to by mnsing and singre teat das privitul exgleing de the sengs th\n",
      "ficle controst informicianded overen depanded to it du greatoct ot the of they w\n",
      "hard playerd mmonately forced workshy enerres albing hears telitivian for remort\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3100: 1.642671 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3200: 1.643436 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3300: 1.625977 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3400: 1.643359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3500: 1.620983 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3600: 1.626812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3700: 1.644202 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3800: 1.644690 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3900: 1.660272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4000: 1.633626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "on incuete to jurician cistimal artiveds for to made are budy phas cauly see spe\n",
      "gree vist intries are ware s arring the x to of clusure and reffraction aftermag\n",
      "usa her constitutly in the marg contin to boylts matherndamm liceshes consholely\n",
      "kan and abu eight insidersua acceasion darf on gol eashine is cell first mattur \n",
      "formation which a linking however meq arpoobian two two beinst and so and invelu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4100: 1.642572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4200: 1.649066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4300: 1.646112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4400: 1.640284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4500: 1.645127 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4600: 1.631060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4700: 1.613475 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4800: 1.616762 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4900: 1.602992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5000: 1.632119 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "bia gunsthy accueath techan the and ode yow the a historyed while dikpon althoug\n",
      "riffics betanded commusing servedred stle varko is usson ofter anda bechint bya \n",
      "teanims such as prosintary main va was matzaly it james the was adid batazabls c\n",
      "us jower a nota combuss islans x his a liversed theollyhinting johan irmat of is\n",
      "wayo and treazes redeadancy holdo came and aretion schoar army direrack wals may\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5100: 1.626064 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5200: 1.608047 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5300: 1.635341 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5400: 1.617751 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.606764 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5600: 1.585268 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5700: 1.589757 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5800: 1.579290 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5900: 1.598273 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6000: 1.586536 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "y germany book one nine five f groups engriant decided eight one zero lev monky \n",
      "ky yuilt french after to his emmerous analembases by this two the dies hang f ra\n",
      "an umerical and entidementia deacher comerator fnx pranser gengy attempul his ub\n",
      "metory in tort green outsidunamizarl short two zero zero two seven eise in the a\n",
      "restex from philist haw ewate airsor not in the birdinmforn arck entirely the po\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6100: 1.599407 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6200: 1.607961 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6300: 1.585604 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6400: 1.589555 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.597177 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6600: 1.584123 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6700: 1.580474 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6800: 1.597963 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6900: 1.593187 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7000: 1.585279 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "ostender butcher standalchiltane ulsed and blumed the franch respered on brank i\n",
      "liged enreking the lacaroor a photescataly metre was mebses moder in bliey yearl\n",
      "s also one eight hillwa lik one eight two zero zero seven thichabatorial sprunge\n",
      "us berays becabour of the heary den d one d one nine five six gupbers nine knowe\n",
      "jabated transold after all liver colvence to britishibration republicang nationa\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run() # Initially   tf.global_variables_initializer().run()  print('Initialized')\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for each, and variables that are 4 times larger.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Problem 1 You might have noticed that the\"\"\"\n",
    "\"\"\" definition of the LSTM cell involves 4 matrix\"\"\"\n",
    "\"\"\" multiplications with the input, and 4 matrix \"\"\"\n",
    "\"\"\"multiplications with the output. Simplify the\"\"\"\n",
    "\"\"\" expression by using a single matrix multiply\"\"\"\n",
    "\"\"\" for each, and variables that are 4 times larger.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters  \n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(1, 4, smatmul)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295625 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "ynlfeihl qci fgez sjforrjctlrlb  rray l hug vsnntwe  qmttspsawnwm ocnt jwxruwoiq\n",
      "a  gwtsbis yryngkn cpf nc xee otew  lreer u uac ve ohdo e esqc naltwxmbaro sabr \n",
      "c eoxgpejrf ohvot k mdqdagzk d ypm  p fsiyav h au utct sf dsar cavbdsk tmgiona l\n",
      "hmghenfatj uiinphjyqnxw mrpsnfhrb in ticek slxhdsyzzisxyevpeu  hxe  ihlnfee ee g\n",
      "ropejrmzdvgayvtt wxsgs ennq ps   eoneexkjizwovkycaapovdozawjlsec  zfxkjes a e si\n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.582526 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.59\n",
      "Validation set perplexity: 10.82\n",
      "Average loss at step 200: 2.251404 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.10\n",
      "Validation set perplexity: 8.95\n",
      "Average loss at step 300: 2.106585 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 400: 2.010065 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 500: 1.960437 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 600: 1.911943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 700: 1.871175 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 800: 1.864741 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 900: 1.830053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 1000: 1.780531 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      "f and co morkal calucic the pasical opt orbarcy at mall railiz pus the four cavl\n",
      "ormes recile coump rals bethrarcy sumicon us bat clatary terlogitions inscente t\n",
      "ly with at goholorg in throught of geraze in invici at call and thad by the prov\n",
      "s are ormen for juyy inden the allation wherx logabs a the lature fout which thb\n",
      "gean ele char zero and the greats crelubgias edsess of actistr of righal velosed\n",
      "================================================================================\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1100: 1.800107 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1200: 1.783340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1300: 1.762453 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1400: 1.751005 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1500: 1.743166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1600: 1.747863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1700: 1.739368 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1800: 1.681817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1900: 1.677354 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.722448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "reat sabelingly volucal revemb freghols a cologe also meanuse diffedent are is c\n",
      "ic beaguils to thettay refordal the nemparism prexedl phoks the heregatritia has\n",
      "drcryugrign new of pre menst recorgor spagns and endissial greegs petinua six mi\n",
      " gine the compempors wreattrombamed the mesusived area crine one four elevile a \n",
      "riagist ceder in the used to sict used repripacers withetal hup often see possan\n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2100: 1.695551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2200: 1.687024 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2300: 1.689915 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2400: 1.709054 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2500: 1.664929 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2600: 1.626556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2700: 1.660313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2800: 1.661828 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2900: 1.637970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 3000: 1.642048 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "les  if devellatic an how buniam be anotuging the this intr divine na pporical s\n",
      "z that attempty sobned have disproving in the pekian peoplesuwic bua of these be\n",
      "zed sut an elected a scoblan geit confacivile music peb capparting and his s cou\n",
      "ess oston and commondy than inclided creft icon and in she impromimation deernce\n",
      "kline cotlem wigh markes to setianity lexpusine of acarlican invo a instrude end\n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3100: 1.624564 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3200: 1.656803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3300: 1.649419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3400: 1.637454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3500: 1.633034 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3600: 1.657105 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3700: 1.621587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3800: 1.620616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3900: 1.617678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4000: 1.661406 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "un two m o moso rea latlinger of acrospon una innon a sans are convessian full l\n",
      "torwamenising sometations with i set poince j wed poses micros femiles one secon\n",
      "gatiom i way leed new is dift lands of the s bast used bies of owherio a payenth\n",
      "pably balloury didanged of britidenta tames three three of theen trig then culti\n",
      "zers lail at fox domes and for bignin cale for appeprar and as and and serses ni\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4100: 1.629100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4200: 1.651416 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4300: 1.618020 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4400: 1.614209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4500: 1.609946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4600: 1.630564 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4700: 1.598114 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4800: 1.608391 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4900: 1.605342 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5000: 1.607998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "ck after discome breaked on most manysemine ifauclized trwiton that polices evid\n",
      "mettly world authorities lypt arriss from and record states minor is frighentary\n",
      "josive was augu tarked taking that none wor useds of tewon byply in impositionis\n",
      "ons thuse especial pages is also six evolved deck station by a one of reporty es\n",
      "joise svyses and is a several to station knight two s of kore in the interprenci\n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5100: 1.573273 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5200: 1.594444 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.584572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.574347 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5500: 1.582485 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.581143 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.584665 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5800: 1.563996 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5900: 1.557585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6000: 1.608269 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "mess on history more vesics of septows in orial initary a calleding and in exict\n",
      "actic uft india the chorised from unit speasti fromms incis of pan ruch as a are\n",
      "o cultion because near letival composer whens accer loons traditian ibbladia and\n",
      "x absept toboly shoulds while vi of never which example world nucranses of the d\n",
      "zerian woods jassustics commons afting when the dice daffetenscope caslan were n\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6100: 1.604591 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6200: 1.567721 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6300: 1.561932 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.569805 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6500: 1.583005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6600: 1.609546 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.629177 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.579230 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6900: 1.589455 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7000: 1.562220 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "bustic the included other the re the disotcops es not wi hat to cosmos thingstri\n",
      "queth the blate the curce from sacrest in criticies of two six free gresh there \n",
      "ear loed madic resacizite were than increased in a to the sexum in the scroor ca\n",
      "quiram nines hylo of a be caubes of bast uninghology the other abarthents forson\n",
      "thi and fell in the f piita pholing and i years and be at generanement peinniled\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probability for the possible characters, not an embedding.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Problem 2 \"\"\"\n",
    "\"\"\"We want to train a LSTM over bigrams, that is pairs\"\"\"\n",
    "\"\"\" of consecutive characters like 'ab' instead of single\"\"\"\n",
    "\"\"\" characters like 'a'. Since the number of possible bigrams\"\"\"\n",
    "\"\"\" is large, feeding them directly to the LSTM using 1-hot\"\"\"\n",
    "\"\"\" encodings will lead to a very sparse representation that\"\"\"\n",
    "\"\"\" is very wasteful computationally.\"\"\"\n",
    "\"\"\"a- Introduce an embedding lookup on the inputs, and feed \"\"\"\n",
    "\"\"\"the embeddings to the LSTM cell instead of the inputs themselves.\"\"\"\n",
    "\"\"\"b- Write a bigram-based LSTM, modeled on the character LSTM above.\"\"\"\n",
    "\"\"\"c- Introduce Dropout. For best practices on how to use Dropout in \"\"\"\n",
    "\"\"\"LSTMs, refer to this article.\"\"\"\n",
    "\"\"\"Let's first adapt the LSTM for a single character input with\"\"\"\n",
    "\"\"\" embeddings. The feed_dict is unchanged, the embeddings are\"\"\"\n",
    "\"\"\" looked up from the inputs. Note that the output is an array \"\"\"\n",
    "\"\"\"probability for the possible characters, not an embedding.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.290288 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.85\n",
      "================================================================================\n",
      "re varpha fcpxs e ra   xzxcesgl  twtahvgme fsos qtfsraspsxr ilbw wstxvbrnnaa vb \n",
      "k  ae cfauuo uasskkcnta ha nfnndemarsgofwmzmetb femxtz h q lbquaxcnx bvvjbe nkoe\n",
      "xxkiqoikxpe m wuqes  s he   edfreeacs e dz  satsswsterpwerce vb k cd tmkvow a  l\n",
      "j a gfglnbnsoixvlcq yu iviqs ru yhiia   e iz miele o  xaan zuribc  pdoosetxininx\n",
      "tecixl zr hgner if bnr de e tgjvpq zubrunq bsymndrjoclpqpxbdlsacisis esbmsnioqnw\n",
      "================================================================================\n",
      "Validation set perplexity: 19.14\n",
      "Average loss at step 100: 2.300488 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 200: 2.026558 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 300: 1.929438 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 400: 1.882754 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 500: 1.841176 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 600: 1.811400 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 700: 1.808126 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 800: 1.792996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 900: 1.793443 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1000: 1.798865 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "================================================================================\n",
      "ide though the ifar for vall is in the envestation of gy eth land preses in uppl\n",
      "x s flatance come aldus var the radian usid pesselve hichistus hus traffmries to\n",
      "out lstawhal so eld not hazle mon arenisal ding in wor three ft nour greatter of\n",
      "kel to the rejubert molegasis taked such dentricbueld north month the burdena va\n",
      "furatle by it and mezannon a furgize bag spierce lownnd the renturah more single\n",
      "================================================================================\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1100: 1.773962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1200: 1.735325 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1300: 1.730184 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1400: 1.687648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1500: 1.707686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1600: 1.672552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1700: 1.693275 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1800: 1.708519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 1900: 1.681404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2000: 1.671928 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "quistroson no john bobying conservantonian calle directies univeral mace a was h\n",
      "filed wide and vorch dens was with productly internet whikinj a locose and prodi\n",
      "f onutiliate viewere caustranish include was pop can an an in marhever a purent \n",
      "ject the sinceavards of where two bits or defecular leady ald this systembly the\n",
      "wev whe conferenct indectren pirc words sluque an even faling of studons in this\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2100: 1.665321 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2200: 1.661724 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2300: 1.664779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2400: 1.669849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2500: 1.650114 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2600: 1.632425 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2700: 1.672351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2800: 1.655444 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2900: 1.641668 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3000: 1.634881 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "================================================================================\n",
      "vice malow monther authtegaged the one nine two eight five a except cpyarical ba\n",
      "led to only other attain one the discuptured in spea was ang charzume mucould of\n",
      "quained and although lorce its and forset being madels incorementy narly medialt\n",
      "keything of carling wom in p reves be leave and supprince unfercuuman productive\n",
      "vichative in dum the notabure is poins a who the fale a vuley clevule in center \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3100: 1.622070 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3200: 1.624830 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3300: 1.646428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3400: 1.637948 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3500: 1.645321 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3600: 1.628439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3700: 1.658905 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3800: 1.646448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3900: 1.637610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4000: 1.639198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "mk and quit tittan crific deffore two yaglic city wels way the mount to two five\n",
      "rians and a laws the regits just theory two a the irinct by comput wild be cilci\n",
      "zome of appresent instant for obses be decom subden adiencey timbist non one eig\n",
      "ted theater wassael islaul the non the called of indian is the crustines and inc\n",
      "hain recovets to that at and numbra know turnses city in agened thampics milines\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4100: 1.638308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4200: 1.633033 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4300: 1.629375 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4400: 1.598308 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4500: 1.662534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4600: 1.617794 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4700: 1.623106 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4800: 1.627711 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4900: 1.618993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5000: 1.619564 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "jection eath whe ina disement subota is has ponintelation sciences and an to mil\n",
      "onto humkus was junnors usbel the clocal has traise elemeal levented serument fr\n",
      "lding ringmented mytany m set as a science compestity cregited of has enteration\n",
      "or often cities to wasters c dengetent attrnes non zero one nine nine four eight\n",
      "yot as intel due complete used by a reputers the beaur turned to collectrooted h\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5100: 1.580108 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5200: 1.563922 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5300: 1.554765 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5400: 1.542388 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5500: 1.563280 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5600: 1.573332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5700: 1.587361 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5800: 1.537960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5900: 1.523756 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6000: 1.543635 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "================================================================================\n",
      "asy te surp taisause ploth its interfit occevent friendies in one nine of betwee\n",
      "ing at abolbowfore he former itspo galister co ranch its is exace hurly europe r\n",
      "ning cipan from human figury and periencly a darcey at is two four nine factors \n",
      "jess asty left ancienothic pragations one seven one was value anuals current bit\n",
      "ic said tofficie victivily tookism espiethys dordaboration succry arch hare six \n",
      "================================================================================\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6100: 1.544378 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 6200: 1.545877 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6300: 1.560470 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 6400: 1.573031 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6500: 1.576953 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6600: 1.550223 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.580912 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6800: 1.540470 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6900: 1.548031 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 7000: 1.537474 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "kew more other in the telab a center a and groups we whice often on the one zero\n",
      "key itchabling ternan security disparty ranged to schilds have force the bool fi\n",
      "xip is whoposng desansis boor nove from languagirational communistic cost articl\n",
      "wardsan bfome as ofrest germeticing cholars of rick leading and one as of an hum\n",
      "ponly locat boardoss america would t the earnist mazing one one with mas ver ast\n",
      "================================================================================\n",
      "Validation set perplexity: 4.17\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' characters (not bigrams).'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We can now use bigrams as inputs for the training.\"\"\"\n",
    "\"\"\" Here again, the feed_dict is unchanged, the bigram \"\"\"\n",
    "\"\"\"embeddings are looked up from the inputs. The output \"\"\"\n",
    "\"\"\"of the LSTM is still a probability array of the possible\"\"\"\n",
    "\"\"\" characters (not bigrams).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #print(i.get_shape())\n",
    "    #print(i)\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    #print(logits.get_shape())\n",
    "    #print(tf.concat(0, train_labels).get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300293 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "================================================================================\n",
      "uhltxjteowng z hlnxxit huhrniudx  yevai uqwah pfurv  rhcm areh t pnxonxusxhg  vdk\n",
      "ucenm f z nt goegrd oag     nkrk edcr rgthtq rxh  pcee hutstm yn orxxesmeza  r re\n",
      "nizofb ht  m s zi optmsyt h soll tmvsgt eojboena n w njttof zqfdquljozz jri legc \n",
      "cucd kv oareokeseugnottnient n  tjuyyzliyeoou zlwo riajesmfqlizsahtqsd    insli x\n",
      "lpnknnsyaozrtqaadse  sgbod dj ihkiibroculx ultxo cgdnhbfynfzggg uysr u clleahwt v\n",
      "================================================================================\n",
      "Validation set perplexity: 19.21\n",
      "Average loss at step 100: 2.255327 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 8.74\n",
      "Average loss at step 200: 1.963605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 300: 1.862077 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 400: 1.818139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 500: 1.753959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 600: 1.737580 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 700: 1.758884 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 800: 1.736638 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 900: 1.715863 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 1000: 1.713575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "oztures hed to the advance on glauchers is genery ii only and plaal have on with \n",
      "kbutiter wordisathe under itil are that auguus to mack losuhand ags two one nine \n",
      "ve zero one five three born mohvherre seven many meings as their and k the mmnwar\n",
      "ft misn and but john in rejes or the to crownse is the five follows attaktnationa\n",
      "uah but woukld ort reagine is traid their was incod the electron for worts office\n",
      "================================================================================\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1100: 1.702781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 1200: 1.676186 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 1300: 1.678927 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 1400: 1.713438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 1500: 1.688798 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 1600: 1.680352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 1700: 1.657307 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1800: 1.644317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 1900: 1.652501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 2000: 1.657716 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "port becoes stantly swesed pass carvely masted from head games and is nasa goetis\n",
      "ih three seven five one cloth on the irattomost are exactions adusism appeal acce\n",
      "pyter shammes dencce pronois it of ms can religious in one seven five zero three \n",
      "een it wowlrs also relayaking muslim of c a callexis year various and ment streas\n",
      "dan critanktable in one three and and fourthought the chiladine that is carbirafe\n",
      "================================================================================\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 2100: 1.649741 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 2200: 1.646774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2300: 1.622394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 2400: 1.641523 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2500: 1.631469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 2600: 1.632297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 2700: 1.631499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2800: 1.626082 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2900: 1.610552 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 3000: 1.630915 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "uhlander ansian playing a floimmosted about paused social of buislandice that ide\n",
      " these studied camedokoss anzity capetived escainted red entinglation world other\n",
      "ekant hoted killing town propose came englight the two zero s s to bon of lation \n",
      "bvnals age like publise four two probetre ults time of nio iromant of fhination w\n",
      "wledgeinneriocill its officially form informility official inctivish factory is u\n",
      "================================================================================\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 3100: 1.650815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3200: 1.632270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 3300: 1.611711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 3400: 1.667291 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3500: 1.616248 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3600: 1.613429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 3700: 1.591946 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 3800: 1.622023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 3900: 1.639044 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 4000: 1.608469 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "jcid was of charlanese of new with mets so members within one eventh marsher acti\n",
      "cles sot fundard cast for more united whent w gamer sughered storgeenbm the form \n",
      "sjs and two two zero two are is a to is a telhsirs of the deritical six six six z\n",
      "xy lecudaeing they freedians to such the six th two eight eight seven in step coa\n",
      "fpt and cas aprool multam corees hollattor cellins was forgimes queer in dilese p\n",
      "================================================================================\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4100: 1.616904 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4200: 1.602829 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 4300: 1.601040 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 4400: 1.595878 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 4500: 1.574016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4600: 1.598917 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 4700: 1.621750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 4800: 1.605736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 4900: 1.604294 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 5000: 1.591304 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "dt much impations is markent holog to has vis supporter featrically digitterary a\n",
      "mhonion joself tibet of prayer dolectroped of warne five esself boy this party no\n",
      "abaltable asistrically of defens actions arguat howeverpiended s hass social unit\n",
      " dief howed in orance has party taketer and perportine of the and camp solent pro\n",
      "rnation europot hool feat wide more adminaturaly explott bough and language years\n",
      "================================================================================\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 5100: 1.555158 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 5200: 1.547982 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 5300: 1.557300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 5400: 1.593556 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5500: 1.547024 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 5600: 1.571001 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 5700: 1.540884 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 5800: 1.576230 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 5900: 1.541638 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 6000: 1.563284 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "brandhao musicf the nasally setting michay trarge two zero two seven work wlvars \n",
      "zz american the cuu becamationa affection osmeoples are metric rought to geograti\n",
      "ging bours killing palace is non ugued tempted made agolin six states at in the b\n",
      "zina drive force flows is an inpution sweaed with the kacksaavibined the five jot\n",
      "ula storied group two centrouble which has how is been force fortance spocks anot\n",
      "================================================================================\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 6100: 1.553511 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 6200: 1.548751 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 6300: 1.567126 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6400: 1.561419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 6500: 1.547458 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 6600: 1.578154 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 6700: 1.555767 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 6800: 1.574632 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 6900: 1.553828 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 7000: 1.529382 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "================================================================================\n",
      "cq shport in one nine two zero s interiously used the engiseens adclein this and \n",
      "ease of aughtegorid marrishon into two the moveknow age broad sucpure empironorm \n",
      "ne nine eight six one sure englishhdivincipacitations intel of gior that and she \n",
      "pvirddina and influence the cinition from etce of is by two five one six eight ze\n",
      " his that cultures portunt related d who group exceess terms of honoxe cants the \n",
      "================================================================================\n",
      "Validation set perplexity: 6.80\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's try the dropout, in the inputs/ouputs only, not between to cells.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"It works, but the validation perplexity is a bit worst.\"\"\"\n",
    "\"\"\"Let's try the dropout, in the inputs/ouputs only, not between to cells.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.309317 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.37\n",
      "================================================================================\n",
      "qz  mherlqefggu ae hdmqnen iaoelmc eflm ci ariaqevs bnne d stfcsvm lvtqkcsgvcmjuo\n",
      "qvqa lsoensgps pur uxtc kj dinahtu ihejg yapy nzkesehsetvdz xhu lfuepbsledw ue i \n",
      " a p  pn erkmscs remxz toocncjvntuuinm v diarmwdgr qmzr  q f es ntmu   heuiccwtgw\n",
      "dksbaufanp  igiatragaunzt  cntkshpeuqdmdleifttoieg xle bc eeo b n euk yiugnm it  \n",
      "nmy t rnelnehwrl n s nwt locsanwppaazgshxnkasojgrs en ocaceescg rofwx psferaco n \n",
      "================================================================================\n",
      "Validation set perplexity: 19.88\n",
      "Average loss at step 100: 2.302575 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.03\n",
      "Validation set perplexity: 9.17\n",
      "Average loss at step 200: 1.960589 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 300: 1.851570 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 400: 1.800679 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 500: 1.790711 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 600: 1.779525 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 700: 1.743760 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 800: 1.720506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 900: 1.729418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1000: 1.734242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "by there acropita he island chuaoersetica accommersarinal is have line nine nine \n",
      "bsents puburitfory in the area cate recepted includity descrus in organs seperve \n",
      "zbas byst shronoium more of coas polist in cheversistent one nine one games four \n",
      "nstrubly of peroarder pram production the perstrytance fravek in lickaidas cqls b\n",
      "ns climil operation waticmh the cords the world sfrom of not lintary eide a j it \n",
      "================================================================================\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 1100: 1.719074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 1200: 1.700715 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 1300: 1.673000 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 1400: 1.654912 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 1500: 1.647416 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1600: 1.678074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 1700: 1.671016 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 1800: 1.661367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 1900: 1.660755 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2000: 1.638433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "gcralized or tarted the may of will due jogxpe is and breees vadated axeunded sig\n",
      "ike more of wtoigration lgordomed on two fvithat tentialifore more to to status n\n",
      "yu are on the genize chole of this civersity othern is nation one devertaint cali\n",
      "zv chrotbald it matric mers premian in these five over one nine three proper the \n",
      "equeen is no have one nine six  four one five four of four used two the island st\n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2100: 1.629359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 2200: 1.632646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 2300: 1.643493 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 2400: 1.665308 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 2500: 1.653881 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2600: 1.639634 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2700: 1.609043 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 2800: 1.637087 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 2900: 1.630822 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 3000: 1.644203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "yw stumily but thists the die of roall and misco sael plane of the striln costequ\n",
      "book s sacp lans is the one three list thome sccp abook led the bush supreferego \n",
      "ohi hiddence or reme of sedure of the mexi disbtballand during two cary schlo cla\n",
      "gb ehught nanides the mards us to chaucer recognifically armitreer game universio\n",
      "mfull recures conther three and lexandigh bling a ryal agment of with the reals a\n",
      "================================================================================\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3100: 1.643192 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3200: 1.640074 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3300: 1.630717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 3400: 1.630768 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3500: 1.632314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3600: 1.614218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3700: 1.651483 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 3800: 1.630155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3900: 1.627000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4000: 1.626182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "bgess on there bargocking algium  she arpin forerinagest that where to his scient\n",
      "xzon vespon is of tripply riddle ecodarized to the apps bur to the to bloachion r\n",
      "tment to the mean of terrive ust ar to storitianuagu euses in much begitically ki\n",
      " x sply which or the teling centributher the new asperson medits writtan was trol\n",
      "cz and the residenn to the first chin improven linear gangen you oosite in eage t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4100: 1.621722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4200: 1.628840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 4300: 1.628434 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 4400: 1.603394 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 4500: 1.615536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 4600: 1.621447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4700: 1.626676 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4800: 1.603586 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 4900: 1.619891 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 5000: 1.639784 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "ually intervernal on five two fashaps borly of acomina post for actiopeniast theo\n",
      "ebase  later approxear coaaried in one nine seven cilled chrots foogle discussii \n",
      "vr bergening in two rama to much the euramosocation sistrash examples the churma \n",
      "hwas buddhic binven have between if efficuring the defoundon south of the brought\n",
      "rker gundaps rellost nest in name polay tour tournal quitish gular the poon obt o\n",
      "================================================================================\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 5100: 1.585395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5200: 1.590475 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 5300: 1.593746 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 5400: 1.617660 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 5500: 1.597531 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 5600: 1.591069 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 5700: 1.563452 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 5800: 1.575560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 5900: 1.581971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 6000: 1.572201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "fz remains w french aising complecture when to engthistan to a form in and electr\n",
      "vve ever is along may be churchess dickey john bpbick appers also y been living i\n",
      "dward ried second process and links the city alvement danctional path and hunk in\n",
      "hdowneds a diagned naturage g to the one five two vione players is a dijaost duri\n",
      "wly user obfcite history considered which in radio extractice historical on husso\n",
      "================================================================================\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 6100: 1.568024 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 6200: 1.540213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 6300: 1.566687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 6400: 1.578358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 6500: 1.557168 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 6600: 1.542302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 6700: 1.559922 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 6800: 1.570443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6900: 1.578547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 7000: 1.594357 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "================================================================================\n",
      "mcans irism quannes does are page tall one eight nine nine seven nine nine eight \n",
      "ohan she damy or out withruwte the uk two cricifangian linues that unchurch the c\n",
      "fbally in honsorth people koch alfriencal powernion te shopt in the daski a s a s\n",
      "wbanlay as one nine pamis frang wested in one ss christion re  late lead of hniqu\n",
      "sdals three three one one nine nine five two three eight one nine six a number fo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 7100: 1.559106 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7200: 1.569357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 7300: 1.556737 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 7400: 1.588995 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 7500: 1.588655 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 7600: 1.572893 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7700: 1.604926 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 7800: 1.602680 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 7900: 1.596845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 8000: 1.595439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "ip most is n paranch philowed d a spore and pomal who the requironas iight see wo\n",
      "hs central black to the require laser by bubn a first formay and kions the husban\n",
      "ginsing dority his is some st toltforning existemning byl issue the fbissenerals \n",
      "over to pc orgest vocate four zero zero erentire programment of laws access bot a\n",
      "avis a penine nine six for to be natch being bulgard limital limiladings issia be\n",
      "================================================================================\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 8100: 1.606291 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 8200: 1.565209 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 8300: 1.546328 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 8400: 1.564175 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 8500: 1.570318 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 8600: 1.581052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 8700: 1.578443 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 8800: 1.595195 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 8900: 1.579361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 9000: 1.606765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "hnpace muha but the comparthored court university halp betwork tham even than the\n",
      "ww orthodox three five on a day of this stramans conclulan howes loft spently gre\n",
      "hbu adbody hemently genanon the three one one zero three zero it oness comics a c\n",
      "reck expanding to be they home had world in judge unlirst of capting brogpisis el\n",
      "dfsrh consider i region wing laural sincles are amanduary in the coue almost crea\n",
      "================================================================================\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 9100: 1.588906 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 9200: 1.602102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 9300: 1.601868 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 9400: 1.598602 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 9500: 1.582901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 9600: 1.564964 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 9700: 1.575996 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 9800: 1.565579 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 9900: 1.573935 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 10000: 1.546211 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "================================================================================\n",
      "rchally that formated runnaded they formulfdhong of the loding conferred interati\n",
      "vun acceptic throughout extendered nationation antionated formal articles tense e\n",
      "othanted requence and three three the operbird the keeph a argument insurre which\n",
      "consity frequelktanked proviously ises porter on the himu lecgian to the poet for\n",
      "kwhen have foundind to unto the beings the antism chemis one nine nine seven one \n",
      "================================================================================\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 10100: 1.545795 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 10200: 1.537655 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 10300: 1.533409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 10400: 1.540250 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 10500: 1.558021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 10600: 1.560484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 10700: 1.529525 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 10800: 1.532182 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 10900: 1.513642 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 11000: 1.571124 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      " containsteal senterway taury took and differ assember an logicans plant was worl\n",
      "hly youndation in oried temptionship upho someigns in say operation ofrome if the\n",
      "zwelly not as the companetweep televels are asiao jewed with at whies order that \n",
      "ke gradical domes fall his mulator of hbard ming hers assfuck romittell maryda wa\n",
      "h understine and monthematics take the evangdom or foundatured allowing the and g\n",
      "================================================================================\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 11100: 1.554208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 11200: 1.592207 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 11300: 1.564089 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 11400: 1.555350 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 11500: 1.583538 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 11600: 1.551636 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 11700: 1.567496 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 11800: 1.590160 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 11900: 1.613333 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 12000: 1.595933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "gqtbay critices the ladr two five hod widt of itsgypt hislands has anesever  s in\n",
      "lurose for perfice of kerbus of an economy from the distribional the follows last\n",
      "u tanete in the serates five one nine eight eight zero eight two zero zero eight \n",
      "sqside body and woned on absoluthered feminiation mathetic n red indened and has \n",
      " justian population it very the unix s davs most of the unomper during to be an g\n",
      "================================================================================\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 12100: 1.607073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 12200: 1.623994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 12300: 1.613138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 12400: 1.590682 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 12500: 1.581906 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 12600: 1.554250 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 12700: 1.574124 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 12800: 1.570203 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 12900: 1.617752 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 13000: 1.587571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "nowne even any of f that mauritish assotive capitate cnn cropreson the old a cont\n",
      "dy and gramer framerity bach court are and and roman colump in the folk scwerbrab\n",
      "mcg myat consuscooned hered as periable behtmltj rael use of the when markets and\n",
      "minifese the of completes hat scene fate of bram alson are susched and writers fa\n",
      "the father barousan union runter were at chromations to regulary with mainly of t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 13100: 1.617519 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 13200: 1.605680 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 13300: 1.619341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 13400: 1.608861 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 13500: 1.615562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 13600: 1.621197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 13700: 1.607684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 13800: 1.587624 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 13900: 1.584298 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 14000: 1.574275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "hfered divisions spok japaide into s which  strongent he exchams from the uniffic\n",
      "wbilion five four five nine four ford any s africh latischronulta hazilmt what to\n",
      "mostler animal from three eight an evil tory most judges in thunk has traved the \n",
      "wfour social in the without in myce by meands to are from a liveation one zero co\n",
      "cycs played it is nom weds to us influres to the british lav yugjta three zero it\n",
      "================================================================================\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 14100: 1.579978 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 14200: 1.581571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 14300: 1.596328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 14400: 1.591676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 14500: 1.568121 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 14600: 1.564582 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 14700: 1.594976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 14800: 1.577738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 14900: 1.595816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15000: 1.602376 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "uf securnat set re programminish substem the influcing away one two doctring libe\n",
      "zpyritters and computer of ernant the idea new a michael elance of by emperoodrio\n",
      "wf any to the certas by or the been witharland books the that them chrbii life of\n",
      "ks in mon at a divind are on within leaked film to the perfroms on saths lgrement\n",
      "ed as war owners by exise on supply winfest thew the feminism from hopp descrics \n",
      "================================================================================\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 15100: 1.574020 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 15200: 1.535721 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 15300: 1.536644 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 15400: 1.545640 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 15500: 1.536365 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 15600: 1.540696 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 15700: 1.536565 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 15800: 1.546533 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 15900: 1.560103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 16000: 1.571460 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "================================================================================\n",
      "idented humber severay cross the working sketter kingdom or joyi such as actured \n",
      "zqyamical and of logoo kyarch spaces canadialism sao extracties totranscriptor al\n",
      "vg treasons that the traditions also cards false volume the g was interpresenter \n",
      "vx clawaid fric in each w winne could berg to totari econamistrical scients vic c\n",
      "sgnom one nine volinibero known thanre was millional e became actric all mnnigbho\n",
      "================================================================================\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 16100: 1.555084 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 16200: 1.573963 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 16300: 1.556659 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 16400: 1.575051 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 16500: 1.529106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 16600: 1.539674 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 16700: 1.521430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 16800: 1.529881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 16900: 1.523791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 17000: 1.535627 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "================================================================================\n",
      "gaties was is but the ratch at other the cates weast a combined to three th great\n",
      "gots in one eight daneks however up electrice minius used alightnsdisplievation o\n",
      "nting an importies seven sulter runred also is supplied state in the minit second\n",
      "zholong water in the gehave the six general mathirborcdicia breach nischuon town \n",
      "his reservickly into deten nets them for relationing with a by of the mains for a\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 17100: 1.536784 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 17200: 1.596026 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 17300: 1.566276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 17400: 1.580026 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 17500: 1.569882 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 17600: 1.568442 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 17700: 1.581759 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 17800: 1.584561 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 17900: 1.566792 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 18000: 1.563994 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "txred fight of ridge lb procession theori tled the nagnizing of one nine one sah \n",
      "dkar germanual assed it and cathology adrian wasseosopher and the prela been pain\n",
      "mt n positive perty warm purip luony mean yal uponne the seat to though results c\n",
      "nxl deast in temperous for medium blamr gage of lod mysteries a lavy land justs o\n",
      "akes havanker while to unial were example to us of destroyish lailjacas of hvarro\n",
      "================================================================================\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 18100: 1.556228 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 18200: 1.576467 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 18300: 1.570853 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 18400: 1.574641 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 18500: 1.526239 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 18600: 1.539793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 18700: 1.530087 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 18800: 1.520605 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 18900: 1.533963 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 19000: 1.540340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "ny other pows alf there isgend to gamences and planed due to the made increased a\n",
      "vm marlosaeven them chaustry while synortionate makens of the orwaithave of a riv\n",
      "subrinany is among at the were most in monding befil every as the city ay boat si\n",
      "czech of the courcution generals ell squaries being franc magy s boxes the poyxim\n",
      "vqrgest packing of end paping differes of later malciyal rea ess exchrisia was fo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 19100: 1.524983 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 19200: 1.532483 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 19300: 1.502635 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 19400: 1.511738 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 19500: 1.529313 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 19600: 1.506481 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 19700: 1.532203 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 19800: 1.533095 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 19900: 1.532658 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 20000: 1.491227 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "================================================================================\n",
      "gman wider batmental eticular watk which other chicago dison apology still ism ba\n",
      "en church thered in the fanh yarks and a dianical lad country was jewish nexame e\n",
      " between fok the right group in one one seven five deed the clicity and x on the \n",
      "bdic node the existed on be seven three zero two two zero nine two four one dasen\n",
      "gzedicine of that modern sandsome when that portain with the nights of a sunned t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 20100: 1.518760 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 20200: 1.503906 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 20300: 1.488568 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 20400: 1.520173 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 20500: 1.527754 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 20600: 1.532736 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 20700: 1.515969 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 20800: 1.514754 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 20900: 1.540921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 21000: 1.536541 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "und haltumer to clo one eight zero eight frestuation with theory to indical josef\n",
      " z persus extran eiboyed by oper impersm also also lossoff of god crall findic go\n",
      "ikermality white further of measurged as the with boxyne in leader htings in the \n",
      "hreed for weekegangdo had market of referenc thingsts programme six two buid cane\n",
      "vnes jese in currences of servinmacking in the g for a starts power of the tkpang\n",
      "================================================================================\n",
      "Validation set perplexity: 6.29\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Even with more steps, the final perplexity is not better.\"\"\"\n",
    "\"\"\" Since I do not know what to expect, and since I do not see \"\"\"\n",
    "\"\"\"any obvious issue (the perplexity being consistent), I'm stuck.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Problem 3 (difficult!)\"\"\"\n",
    "\"\"\"Write a sequence-to-sequence LSTM which mirrors all \"\"\"\n",
    "\"\"\"the words in a sentence. For example, if your input is:\"\"\"\n",
    "\"\"\"the quick brown fox\"\"\"\n",
    "\n",
    "\"\"\"the model should attempt to output:\"\"\"\n",
    "\"\"\"eht kciuq nworb xof\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Refer to the lecture on how to put together a \"\"\"\n",
    "\"\"\"sequence-to-sequence model, as well as this article\"\"\"\n",
    "\"\"\" for best practices.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
